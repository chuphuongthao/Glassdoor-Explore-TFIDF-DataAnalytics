We are looking to hire a data engineer to join our team. You will take responsibility for developing out data strategy and management to power new AI-oriented tools and predictive models being applied to healthcare data sets. This will include developing data environments, pipelines, and overall data management. To do well in this role you need a very fine eye for detail, experience in data engineering, and a deep understanding of the enterprise data engineering tools and best practices. This is a contract position with the possibility of moving to full-time.About Refactor HealthOur goal at Refactor Health is to transform how clinical insights are created. As a healthcare AI startup, we develop platforms to reimagine the way we acquire, curate, and learn from healthcare data.Data Engineer ResponsibilitiesShip high-quality, well-tested, secure, and maintainable codeDevelop data models for graph databasesBuild ETL pipelines to surface data from various relational and non-relational sources and create graph databases (e.g. Neo4J, Cosmos DB, GraphX) and optimize for scalability and performanceCreate and maintain environments to support rapid development of AI and ML models, training, and proof-of-concept productsDesign, develop, and maintain data pipelines and back-end services for data ingestion, feature engineering, and quality control/monitoringWork closely with data science engineers to understand pipeline requirements, and DevOps engineers to facilitate CI/CD deployment and create effective feature and technology roadmapsCollaborate with infrastructure engineers to ensure data security and isolation in compliance with regulatory requirementsAuthor technical design documents, architecture diagrams, and other standard documentationData Engineer Skills and QualificationsAbility to partner with semi-technical / non-technical colleagues, from data scientists to C-level executives, and transform business requirements into technical solutionsFluent in multiple development languages (e.g. SQL, Cypher/Gremlin, Python, TypeScript)Experience with PySpark frameworkExpertise in relational or graph data modeling, schema design, and normalizationBackground in one or more of the following areas of expertise:Software engineering and modern application developmentData management, integration, and analysisSemantic technologies, linked data, knowledge graphs and ontology engineeringArtificial intelligence, knowledge representation and machine learningNatural language processing and information retrievalExperience with one or more of the following in an enterprise/commercial setting:Architecting, building, and maintaining end-to-end, high-throughput, secure data systems and their supporting services (job schedulers, message queues, etc.)Working with distributed systems architectures (PySpark preferred)Utilizing a variety of data stores, including RDBMS (SQL), graph networks (e.g CosmosDB) and searchable document DBs (eg. Elasticseach)Developing data pipelines with cloud services and tools (e.g. Azure, AWS)Using profiling tools and metrics to enable performance improvementsUsing DAG/workflow management tools (Airflow)Deploying services in containerized environments (Kubernetes, Docker)Incorporating stream/queue tools in application design (RMQ, Kafka)Developing for continuous integration and automated deploymentsData Engineer RequirementsBS/MS in Computer Science or a related technical fieldEnterprise/commercial work experience (5+ years) as a data engineerProduction experience developing with big data frameworks, such as SparkProduction experience with standard search frameworks, such as ElasticsearchJob Types: Full-time, ContractWork Location: Remote 