Equivalent Experience

Description:
Data Engineering and Bioinformatics is part of an enterprise effort to enable data-driven decisions at xyz. The partner-centric group is embedded with stakeholders across the entire pipeline value chain from Discovery, Translational, Clinical Innovation to Commercial and more. Our focus is to enable Data Science with data in a rapid, exploratory environment and ensure data is democratized across the organization. We constantly push ourselves to innovate, improve our practices, and bring value to our stakeholders.
Your expertise
The successful candidate will contribute to the mission of the global data engineering function and be responsible for many aspects of data including architecture, access, classification, standards, integration, pipelines and visualization. Although your role will involve a diverse set of data-related responsibilities, your expertise will be on transformation of data using programming languages such as Python or R on challenges varying from complex aggregations, wrangling, quality control, to calculations. You will use a variety of data frame, quality and transformation frameworks such as Pandas, Deequ, Great Expectations, Pandera and SQL-based transformations such as dbt, and have excellent mastery over code architecture, including establishing modular patterns of code transformations and balancing with specific one-offs. You may also consider machine learning approaches to do advanced data transformations based on complex patterns. You will be required to share your expertise of Python and related libraries throughout the group. Your ultimate goal will be to place data at the fingertips of stakeholders and enable science to go faster. You will join an enthusiastic, agile, fast-paced and explorative global data engineering team.
General responsibilitiesDesign, implement and manage ETL data pipelines that ingest vast amounts of commercial and scientific data from public, internal and partner sources into various repositories on a cloud platform (AWS)Enhance end-to-end workflows with automation that rapidly accelerate data flow with pipeline management tools such as AirflowImplement and maintain databases for raw and processed commercial and scientific dataInnovate and advise on the latest technologies and standard methodologies in Data Engineering and be able to identify software solutions that can address hurdles in data enablementManage relationships and project coordination with external parties such as Contract Research Organizations (CRO) and vendor consultants / contractorsDefine and contribute to data engineering practices for the group, including expertise in your focus area, and establishing templates and frameworks, determining best usage of specific cloud services and tools, and working with vendors to provision cutting edge tools and technologiesCollaborate with data scientist leads to determine best-suited data enablement methods to optimize the interpretation of the data, including creating presentations and leading tutorials on data usage as appropriateApply value-balanced approaches to the development of the data ecosystem and pipeline initiativesProactively communicate data ecosystem and pipeline value propositions to partnering scientific collaborators
RequirementsBS/MS in Computer Science, Bioinformatics, or a related field with 8+ years of software engineering experience or a PhD in Computer Science, Bioinformatics or a related field and 5+ years of software engineering experienceExcellent skills and deep knowledge in Python, Pythonic design, object-oriented programming, functional programming and design patterns is a must. Experience with R a plusExcellent skills and deep knowledge of data quality and transformation libraries such as Pandas, PyDeequ, Great Expectations, Pandera, and/or SQL transformation tools such as dbtSolid understanding of ETL pipeline, automation and workflow managements tools such as Airflow, AWS Glue, Amazon Kinesis, AWS Step Functions, and CI/CDSolid understanding of databases such as Postgres, Elasticsearch, Redshift, and Aurora, including distributed database design, SQL vs. NoSQL, and database optimizationsSolid understanding of AWS cloud computing services such as Lambda functions, ECS, Batch and Elastic Load Balancer and other compute frameworks such as Spark, EMR, and DatabricksProficiency with modern software development methodologies such as Agile, source control, project management and issue tracking with JIRAProficiency with container strategies using Docker, Fargate, and ECRProficiency with Linux and shell scriptingExperience working with GxP and non-GxP data a plus
Skills:
Aws, Cloud, Python, Aws services, shiny apps
Top Skills Details:
Aws,Cloud,Python,Aws services,shiny apps
Additional Skills & Qualifications:
top 3
AWS ONLY!!!Shiny Apps or JavascriptPython
hybrid out of NJ, Direct Placement, can sponsor
Experience Level:
Expert Level
About TEKsystems:
We're partners in transformation. We help clients activate ideas and solutions to take advantage of a new world of opportunity. We are a team of 80,000 strong, working with over 6,000 clients, including 80% of the Fortune 500, across North America, Europe and Asia. As an industry leader in Full-Stack Technology Services, Talent Services, and real-world application, we work with progressive leaders to drive change. That's the power of true partnership. TEKsystems is an Allegis Group company.
The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. 